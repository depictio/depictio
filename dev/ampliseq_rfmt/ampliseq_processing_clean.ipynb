{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AmpliseQ Data Processing - Clean Generic Version\n",
    "\n",
    "Generic pipeline for processing AmpliseQ results with configurable paths and sample filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# Define your input/output directories here\n",
    "INPUT_DIR = Path(\"/Users/tweber/Data/TREC/nf-core-ampliseq-results-downstream-test-090925\")\n",
    "OUTPUT_DIR = Path(\n",
    "    \"/Users/tweber/Gits/workspaces/depictio-workspace/depictio/depictio/api/v1/configs/ampliseq_dataset\"\n",
    ")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optional: Define sample whitelist (None = process all samples)\n",
    "SAMPLE_FILTER = None  # Example: ['SRR10070130', 'SRR10070131', 'SRR10070132']\n",
    "SAMPLE_FILTER = [\n",
    "    \"sample_112506262\",\n",
    "    \"sample_112506283\",\n",
    "    \"sample_112506284\",\n",
    "    \"sample_112506319\",\n",
    "    \"sample_112506337\",\n",
    "    \"sample_112506346\",\n",
    "    \"sample_112506377\",\n",
    "    \"sample_112506401\",\n",
    "    \"sample_112506413\",\n",
    "    \"sample_112506450\",\n",
    "    \"sample_112506468\",\n",
    "    \"sample_112506477\",\n",
    "    \"sample_112506504\",\n",
    "    \"sample_112506519\",\n",
    "    \"sample_112506527\",\n",
    "    \"sample_112506564\",\n",
    "    \"sample_112506588\",\n",
    "    \"sample_112506600\",\n",
    "    \"sample_112506637\",\n",
    "    \"sample_112506655\",\n",
    "    \"sample_112506664\",\n",
    "    \"sample_112506555\",\n",
    "    \"sample_112506713\",\n",
    "    \"sample_112506722\",\n",
    "    \"sample_112506877\",\n",
    "    \"sample_112506901\",\n",
    "    \"sample_112506913\",\n",
    "    \"sample_112494061\",\n",
    "    \"sample_112494065\",\n",
    "    \"sample_112494074\",\n",
    "    \"sample_112494080\",\n",
    "    \"sample_112494083\",\n",
    "    \"sample_112494092\",\n",
    "    \"sample_112494098\",\n",
    "    \"sample_112494099\",\n",
    "    \"sample_112494545\",\n",
    "    \"sample_112494539\",\n",
    "    \"sample_112494536\",\n",
    "    \"sample_112494563\",\n",
    "    \"sample_112494557\",\n",
    "    \"sample_112494554\",\n",
    "    \"sample_112494578\",\n",
    "    \"sample_112494572\",\n",
    "    \"sample_112494569\",\n",
    "    \"sample_112507248\",\n",
    "    \"sample_112507242\",\n",
    "    \"sample_112494018\",\n",
    "    \"sample_112499299\",\n",
    "    \"sample_112499305\",\n",
    "    \"sample_112499308\",\n",
    "    \"sample_112499404\",\n",
    "    \"sample_112499398\",\n",
    "    \"sample_112499374\",\n",
    "    \"sample_112494076\",\n",
    "]\n",
    "\n",
    "# Condition column name - specify the metadata column to use for grouping samples\n",
    "# Common values: 'condition' (new experiments), 'habitat' (old experiments)\n",
    "# Set to 'auto' to auto-detect (tries 'condition' first, then 'habitat')\n",
    "CONDITION_COLUMN_NAME = 'condition'  # Options: 'auto', 'condition', 'habitat', or any custom column name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Process Alpha Diversity (Faith PD)\n",
    "\n",
    "Converts alpha rarefaction data from wide to long format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_faith_pd(\n",
    "    input_dir: Path,\n",
    "    output_dir: Path,\n",
    "    sample_filter: Optional[list[str]] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process Faith PD alpha diversity data from wide to long format.\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Path to ampliseq results directory\n",
    "        output_dir: Path to save output file\n",
    "        sample_filter: Optional list of sample IDs to include\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame in long format with columns: sample, depth, iter, faith_pd\n",
    "    \"\"\"\n",
    "    # Read input file\n",
    "    input_file = input_dir / \"qiime2/alpha-rarefaction/faith_pd.csv\"\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Apply sample filter if provided\n",
    "    if sample_filter is not None:\n",
    "        df = df[df['sample-id'].isin(sample_filter)]\n",
    "    \n",
    "    # Melt from wide to long format\n",
    "    df_long = df.melt(\n",
    "        id_vars=['sample-id'],\n",
    "        var_name='iteration',\n",
    "        value_name='faith_pd'\n",
    "    )\n",
    "    \n",
    "    # Extract depth and iteration from column names\n",
    "    df_long['depth'] = df_long['iteration'].str.extract(r'depth-(\\d+)')[0].astype('Int64')\n",
    "    df_long['iter'] = df_long['iteration'].str.extract(r'iter-(\\d+)')[0].astype('Int64')\n",
    "    \n",
    "    # Clean up column names and select final columns\n",
    "    df_modified = df_long.rename(columns={'sample-id': 'sample'})\n",
    "    df_modified = df_modified[['sample', 'depth', 'iter', 'faith_pd']].dropna()\n",
    "    \n",
    "    # Save to output\n",
    "    output_file = output_dir / \"faith_pd_long.tsv\"\n",
    "    df_modified.to_csv(output_file, sep='\\t', index=False)\n",
    "    print(f\"âœ“ Saved Faith PD data: {output_file}\")\n",
    "    print(f\"  Shape: {df_modified.shape}\")\n",
    "    print(f\"  Samples: {df_modified['sample'].nunique()}\")\n",
    "    \n",
    "    return df_modified\n",
    "\n",
    "\n",
    "# Execute function\n",
    "df_faith = process_faith_pd(INPUT_DIR, OUTPUT_DIR, SAMPLE_FILTER)\n",
    "df_faith.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process Taxonomy Barplot Data\n",
    "\n",
    "Converts taxonomy abundance data from wide to long format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_taxonomy_barplot(\n",
    "    input_dir: Path,\n",
    "    output_dir: Path,\n",
    "    level: int = 2,\n",
    "    sample_filter: Optional[list[str]] = None,\n",
    "    condition_col_name: str = 'auto'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process taxonomy barplot data from wide to long format.\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Path to ampliseq results directory\n",
    "        output_dir: Path to save output file\n",
    "        level: Taxonomic level (default: 2 for Phylum)\n",
    "        sample_filter: Optional list of sample IDs to include\n",
    "        condition_col_name: Name of condition column ('auto' to auto-detect, or specific name)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame in long format with columns: sample, taxonomy, count, condition, Kingdom, Phylum\n",
    "    \"\"\"\n",
    "    # Read input file\n",
    "    input_file = input_dir / f\"qiime2/barplot/level-{level}.csv\"\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Apply sample filter if provided\n",
    "    if sample_filter is not None:\n",
    "        df = df[df['index'].isin(sample_filter)]\n",
    "    \n",
    "    # Identify columns: first is sample ID, detect metadata columns, rest are taxonomy\n",
    "    sample_col = df.columns[0]  # 'index'\n",
    "    \n",
    "    # Auto-detect metadata columns at the end\n",
    "    metadata_candidates = ['name', 'condition_binary', 'cycle', 'condition', 'bio_rep', 'habitat']\n",
    "    metadata_cols = [col for col in df.columns if col in metadata_candidates]\n",
    "    n_metadata = len(metadata_cols)\n",
    "    \n",
    "    # Get taxonomy columns (everything between sample ID and metadata)\n",
    "    if n_metadata > 0:\n",
    "        taxonomy_cols = df.columns[1:len(df.columns)-n_metadata]\n",
    "    else:\n",
    "        taxonomy_cols = df.columns[1:]\n",
    "    \n",
    "    print(f\"  Detected {n_metadata} metadata columns: {metadata_cols}\")\n",
    "    print(f\"  Detected {len(taxonomy_cols)} taxonomy columns\")\n",
    "    \n",
    "    # Select taxonomy columns and sample ID\n",
    "    df_samples = df[[sample_col] + list(taxonomy_cols)]\n",
    "    \n",
    "    # Melt to long format\n",
    "    df_modified = df_samples.melt(\n",
    "        id_vars=[sample_col],\n",
    "        var_name='taxonomy',\n",
    "        value_name='count'\n",
    "    )\n",
    "    \n",
    "    original_count = len(df_modified)\n",
    "    \n",
    "    # Rename sample column\n",
    "    df_modified = df_modified.rename(columns={sample_col: 'sample'})\n",
    "    \n",
    "    # Determine condition column to use\n",
    "    if condition_col_name == 'auto':\n",
    "        # Auto-detect: try 'condition' first, then 'habitat'\n",
    "        condition_col = 'condition' if 'condition' in df.columns else ('habitat' if 'habitat' in df.columns else None)\n",
    "    else:\n",
    "        # Use specified column name\n",
    "        condition_col = condition_col_name if condition_col_name in df.columns else None\n",
    "    \n",
    "    # Add condition information\n",
    "    if condition_col:\n",
    "        sample_to_condition = df.set_index(sample_col)[condition_col].to_dict()\n",
    "        df_modified['condition'] = df_modified['sample'].map(sample_to_condition)\n",
    "        print(f\"  Using '{condition_col}' column for sample grouping\")\n",
    "    else:\n",
    "        df_modified['condition'] = None\n",
    "        print(f\"  Warning: Condition column '{condition_col_name}' not found in data\")\n",
    "    \n",
    "    # Parse taxonomy levels\n",
    "    df_modified['Kingdom'] = df_modified['taxonomy'].str.split(';').str[0]\n",
    "    df_modified['Phylum'] = df_modified['taxonomy'].str.split(';').str[1]\n",
    "    \n",
    "    # Filter out invalid taxonomy entries\n",
    "    # Remove rows where:\n",
    "    # - taxonomy is empty or just \";\" or just whitespace\n",
    "    # - Kingdom is empty or is a metadata column name\n",
    "    # - Count is null or zero\n",
    "    metadata_names = set(metadata_candidates)\n",
    "    df_modified = df_modified[\n",
    "        (df_modified['taxonomy'].notna()) &  # Not null\n",
    "        (df_modified['taxonomy'].str.strip() != '') &  # Not empty after stripping\n",
    "        (df_modified['taxonomy'].str.strip() != ';') &  # Not just separator\n",
    "        (df_modified['Kingdom'].notna()) &  # Kingdom not null\n",
    "        (df_modified['Kingdom'].str.strip() != '') &  # Kingdom not empty\n",
    "        (~df_modified['Kingdom'].isin(metadata_names)) &  # Kingdom not a metadata column name\n",
    "        (df_modified['count'].notna()) &  # Count not null\n",
    "        (df_modified['count'] > 0)  # Count greater than zero\n",
    "    ].copy()\n",
    "    \n",
    "    filtered_count = original_count - len(df_modified)\n",
    "    if filtered_count > 0:\n",
    "        print(f\"  Filtered out {filtered_count} invalid/empty taxonomy entries\")\n",
    "    \n",
    "    # Save to output\n",
    "    output_file = output_dir / f\"taxonomy_level{level}_long.tsv\"\n",
    "    df_modified.to_csv(output_file, sep='\\t', index=False)\n",
    "    print(f\"âœ“ Saved taxonomy data: {output_file}\")\n",
    "    print(f\"  Shape: {df_modified.shape}\")\n",
    "    print(f\"  Samples: {df_modified['sample'].nunique()}\")\n",
    "    print(f\"  Taxa: {df_modified['taxonomy'].nunique()}\")\n",
    "    \n",
    "    return df_modified\n",
    "\n",
    "\n",
    "# Execute function\n",
    "df_taxonomy = process_taxonomy_barplot(INPUT_DIR, OUTPUT_DIR, level=2, sample_filter=SAMPLE_FILTER, condition_col_name=CONDITION_COLUMN_NAME)\n",
    "df_taxonomy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process ANCOM Volcano Plot Data\n",
    "\n",
    "Merges ANCOM differential abundance results with taxonomy information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ancom_volcano(\n",
    "    input_dir: Path,\n",
    "    output_dir: Path,\n",
    "    category: str = \"habitat\",\n",
    "    level: str = \"ASV\",\n",
    "    sample_filter: Optional[list[str]] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process ANCOM results and merge with taxonomy data for volcano plots.\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Path to ampliseq results directory\n",
    "        output_dir: Path to save output file\n",
    "        category: ANCOM category (e.g., 'habitat')\n",
    "        level: Taxonomic level (e.g., 'ASV')\n",
    "        sample_filter: Optional list of sample IDs to include (applied to taxonomy table)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: id, taxonomy, Kingdom, Phylum, W, clr\n",
    "    \"\"\"\n",
    "    # Read ANCOM results\n",
    "    ancom_file = input_dir / f\"qiime2/ancom/Category-{category}-{level}/data.tsv\"\n",
    "    df_ancom = pd.read_csv(ancom_file, sep='\\t')\n",
    "    \n",
    "    # Read taxonomy/abundance table\n",
    "    tax_file = input_dir / f\"qiime2/rel_abundance_tables/rel-table-{level}_with-DADA2-tax.tsv\"\n",
    "    df_tax = pd.read_csv(tax_file, sep='\\t')\n",
    "    \n",
    "    # Apply sample filter to taxonomy table if provided\n",
    "    if sample_filter is not None:\n",
    "        # Keep only columns that are in sample_filter (plus metadata columns)\n",
    "        metadata_cols = ['ID', 'Kingdom', 'Phylum', 'Class', 'Order', 'Family', 'Genus', 'Species']\n",
    "        cols_to_keep = [col for col in metadata_cols if col in df_tax.columns]\n",
    "        cols_to_keep += [col for col in df_tax.columns if col in sample_filter]\n",
    "        df_tax = df_tax[cols_to_keep]\n",
    "    \n",
    "    # Create combined taxonomy string\n",
    "    df_tax['taxonomy'] = df_tax['Kingdom'] + ';' + df_tax['Phylum']\n",
    "    \n",
    "    # Merge ANCOM results with taxonomy\n",
    "    df_modified = df_ancom.merge(\n",
    "        df_tax[['ID', 'taxonomy', 'Kingdom', 'Phylum']],\n",
    "        left_on='id',\n",
    "        right_on='ID',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Select and clean final columns\n",
    "    df_modified = df_modified[['id', 'taxonomy', 'Kingdom', 'Phylum', 'W', 'clr']].dropna()\n",
    "    \n",
    "    # Save to output\n",
    "    output_file = output_dir / f\"ancom_{category}_{level}_volcano.tsv\"\n",
    "    df_modified.to_csv(output_file, sep='\\t', index=False)\n",
    "    print(f\"âœ“ Saved ANCOM volcano data: {output_file}\")\n",
    "    print(f\"  Shape: {df_modified.shape}\")\n",
    "    print(f\"  ASVs: {df_modified['id'].nunique()}\")\n",
    "    print(f\"  Phyla: {df_modified['Phylum'].nunique()}\")\n",
    "    \n",
    "    return df_modified\n",
    "\n",
    "\n",
    "# Execute function\n",
    "df_ancom = process_ancom_volcano(INPUT_DIR, OUTPUT_DIR, category=\"habitat\", level=\"ASV\", sample_filter=SAMPLE_FILTER)\n",
    "df_ancom.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "All processed files have been saved to the output directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all output files\n",
    "output_files = sorted(OUTPUT_DIR.glob(\"*.tsv\"))\n",
    "print(f\"\\nOutput files in {OUTPUT_DIR}:\")\n",
    "for f in output_files:\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  - {f.name} ({size_mb:.2f} MB)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depictio-venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
